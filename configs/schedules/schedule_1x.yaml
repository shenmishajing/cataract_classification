optimizer_config:
    optimizer:
        class_path: torch.optim.Adam
        # init_args:
        #     lr: 2.0e-2
        #     momentum: 0.9
        #     weight_decay: 1e-4
    lr_scheduler:
        scheduler:
            class_path: torch.optim.lr_scheduler.MultiStepLR
            init_args:
                milestones: [8, 11]
        warmup_config:
            warmup_iters: 500

trainer:
    max_epochs: 12
